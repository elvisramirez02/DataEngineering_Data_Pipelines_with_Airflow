🎵 Data Pipelines with Airflow

Project: Data Pipelines with Airflow
Hi everyone! 👋

Welcome to my project on Data Pipelines with Airflow. 

Sparkify, a music streaming company, decided it's time to enhance their data warehouse ETL pipelines with automation and monitoring, and they chose Apache Airflow as the tool for the job. 🚀


Project Overview
Sparkify wants to create top-notch data pipelines that are:

Dynamic and built from reusable tasks 🔄
Monitored to ensure smooth operations 📊
Capable of easy backfills 🔙
Focused on data quality, running tests post-ETL to catch any discrepancies 🔍


What I Did
In this project, I worked on:

Building Data Pipelines: Designed and implemented dynamic data pipelines using Apache Airflow.
Automation and Monitoring: Ensured the pipelines are automated and can be easily monitored.
Data Quality Checks: Incorporated tests to validate the data quality after ETL steps.

How It Works
Source Data: The data resides in S3 and consists of JSON logs detailing user activity and JSON metadata about the songs. 🗂️
Processing: The data is processed in Sparkify's data warehouse in Amazon Redshift. 🛢️
