ğŸµ Data Pipelines with Airflow

Project: Data Pipelines with Airflow
Hi everyone! ğŸ‘‹

Welcome to my project on Data Pipelines with Airflow. 

Sparkify, a music streaming company, decided it's time to enhance their data warehouse ETL pipelines with automation and monitoring, and they chose Apache Airflow as the tool for the job. ğŸš€


Project Overview
Sparkify wants to create top-notch data pipelines that are:

Dynamic and built from reusable tasks ğŸ”„
Monitored to ensure smooth operations ğŸ“Š
Capable of easy backfills ğŸ”™
Focused on data quality, running tests post-ETL to catch any discrepancies ğŸ”


What I Did
In this project, I worked on:

Building Data Pipelines: Designed and implemented dynamic data pipelines using Apache Airflow.
Automation and Monitoring: Ensured the pipelines are automated and can be easily monitored.
Data Quality Checks: Incorporated tests to validate the data quality after ETL steps.

How It Works
Source Data: The data resides in S3 and consists of JSON logs detailing user activity and JSON metadata about the songs. ğŸ—‚ï¸
Processing: The data is processed in Sparkify's data warehouse in Amazon Redshift. ğŸ›¢ï¸
